### ðŸ“Œ**Recent papers (2026)**
- [CLIP-Map: Structured Matrix Mapping for Parameter-Efficient CLIP Compression](https://arxiv.org/pdf/2602.05909)
- [When LLaVA Meets Objects: Token Composition for Vision-Language-Models](https://arxiv.org/pdf/2602.04864) 
- [VISTA-Bench: Do Vision-Language Models Really Understand Visualized Text as Well as Pure Text?](https://arxiv.org/pdf/2602.04802)
- [Annotation Free Spacecraft Detection and Segmentation using Vision Language Models](https://arxiv.org/pdf/2602.04699)
- [PIO-FVLM: Rethinking Training-Free Visual Token Reduction for VLM Acceleration from an Inference-Objective Perspective](https://arxiv.org/pdf/2602.04657)
- [Understanding Degradation with Vision Language Model](https://arxiv.org/pdf/2602.04565)
- [When and Where to Attack? Stage-wise Attention-Guided Adversarial Attack on Large Vision Language Models](https://arxiv.org/pdf/2602.04356)
- [Explicit Uncertainty Modeling for Active CLIP Adaptation with Dual Prompt Tuning](https://arxiv.org/pdf/2602.04340)
- [Edge-Optimized Vision-Language Models for Underground Infrastructure Assessment](https://arxiv.org/pdf/2602.03742)
- [Contextualized Visual Personalization in Vision-Language Models](https://arxiv.org/pdf/2602.03454)
- [ReasonEdit: Editing Vision-Language Models using Human Reasoning](https://arxiv.org/pdf/2602.02408)
- [Auto-Comp: An Automated Pipeline for Scalable Compositional Probing of Contrastive Vision-Language Models](https://arxiv.org/pdf/2602.02043)
- [Training-Free Test-Time Adaptation with Brownian Distance Covariance in Vision-Language Models](https://arxiv.org/pdf/2601.23253)
- [One-shot Optimized Steering Vector for Hallucination Mitigation for VLMs](https://arxiv.org/pdf/2601.23041)
- [Triage: Hierarchical Visual Budgeting for Efficient Video Reasoning in Vision-Language Models](https://arxiv.org/pdf/2601.22959)
- [A Comparative Evaluation of Large Vision-Language Models for 2D Object Detection under SOTIF Conditions](https://arxiv.org/pdf/2601.22830)
- [Procedural Knowledge Extraction from Industrial Troubleshooting Guides Using Vision Language Models](https://arxiv.org/pdf/2601.22754)
- [Lingua-SafetyBench: A Benchmark for Safety Evaluation of Multilingual Vision-Language Models](https://arxiv.org/pdf/2601.22737)
- [Gated Relational Alignment via Confidence-based Distillation for Efficient VLMs](https://arxiv.org/pdf/2601.22709)
- [VisionTrim: Unified Vision Token Compression for Training-Free MLLM Acceleration](https://arxiv.org/pdf/2601.22674)
- [Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions](https://arxiv.org/pdf/2601.22150)
- [PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing](https://arxiv.org/pdf/2601.21957)
- [OCRVerse: Towards Holistic OCR in End-to-End Vision-Language Models](https://arxiv.org/pdf/2601.21639)
- [PathReasoner-R1: Instilling Structured Reasoning into Pathology Vision-Language Model via Knowledge-Guided Policy Optimization](https://arxiv.org/pdf/2601.21617)
- [WMVLM: Evaluating Diffusion Model Image Watermarking via Vision-Language Models](https://arxiv.org/pdf/2601.21610)
- [LEMON: How Well Do MLLMs Perform Temporal Multimodal Understanding on Instructional Videos?](https://arxiv.org/pdf/2601.20705)
- [bi-modal textual prompt learning for vision-language models in remote sensing](https://arxiv.org/pdf/2601.20675)
- [MARE: Multimodal Alignment and Reinforcement for Explainable Deepfake Detection via Vision-Language Models](https://arxiv.org/pdf/2601.20433)
- [Non-Contrastive Vision-Language Learning with Predictive Embedding Alignment](https://arxiv.org/pdf/2602.00653)
- [Towards Interpretable Hallucination Analysis and Mitigation in LVLMs via Contrastive Neuron Steering](https://arxiv.org/pdf/2602.00621)
- [Evaluating Large Vision-language Models for Surgical Tool Detection](https://arxiv.org/pdf/2601.16895)
- [PhysicsMind: Sim and Real Mechanics Benchmarking for Physical Reasoning and Prediction in Foundational VLMs and World Models](https://arxiv.org/pdf/2601.16007)
- [Assessing Situational and Spatial Awareness of VLMs with Synthetically Generated Video](https://arxiv.org/pdf/2601.15780)
- [Zero-Shot Product Attribute Labeling with Vision-Language Models: A Three-Tier Evaluation Framework](https://arxiv.org/pdf/2601.15711)
- [Towards Understanding Best Practices for Quantization of Vision-Language Models](https://arxiv.org/pdf/2601.15287)
- [PROGRESSLM: Towards Progress Reasoning in Vision-Language Models](https://arxiv.org/pdf/2601.15224)
- [Does medical specialization of VLMs enhance discriminative power?: A comprehensive investigation through feature distribution analysis](https://arxiv.org/pdf/2601.14774)
- [LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR](https://arxiv.org/pdf/2601.14251)
- [IIR-VLM: In-Context Instance-level Recognition for Large Vision-Language Models](https://arxiv.org/pdf/2601.14188)
- [The Side Effects of Being Smart: Safety Risks in MLLMs' Multi-Image Reasoning](https://arxiv.org/pdf/2601.14127)
- [Insight: Interpretable Semantic Hierarchies in Vision-Language Encoders](https://arxiv.org/pdf/2601.13798)
- [PatientVLM Meets DocVLM: Pre-Consultation Dialogue Between Vision-Language Models for Efficient Diagnosis](https://arxiv.org/pdf/2601.10945)
- [MHA2MLA-VLM: Enabling DeepSeek's Economical Multi-Head Latent Attention across Vision-Language Model](https://arxiv.org/pdf/2601.11464)
- [Unleashing the Capabilities of Large Vision-Language Models for Intelligent Perception of Roadside Infrastructure](https://arxiv.org/pdf/2601.10551)
- [MERGETUNE: Continued fine-tuning of vision-language models](https://arxiv.org/pdf/2601.10497)
- [Urban Socio-Semantic Segmentation with Vision-Language Reasoning](https://arxiv.org/pdf/2601.10477)
- [STEP3-VL-10B Technical Report](https://arxiv.org/pdf/2601.09668)
- [CogRail: Benchmarking VLMs in Cognitive Intrusion Perception for Intelligent Railway Transportation Systems](https://arxiv.org/pdf/2601.09613)
- [PrivLEX: Detecting legal concepts in images through Vision-Language Models](https://arxiv.org/pdf/2601.09449)
- [VideoHEDGE: Entropy-Based Hallucination Detection for Video-VLMs via Semantic Clustering and Spatiotemporal Perturbations](https://arxiv.org/pdf/2601.08557)
- [Cross-modal Proxy Evolving for OOD Detection with Vision-Language Models](https://arxiv.org/pdf/2601.08476)
- [Towards Safer Mobile Agents: Scalable Generation and Evaluation of Diverse Scenarios for VLMs](https://arxiv.org/pdf/2601.08470)
- [Zero-Shot Distracted Driver Detection via Vision Language Models with Double Decoupling](https://arxiv.org/pdf/2601.08467)
- [CoMa: Contextual Massing Generation with Vision-Language Models](https://arxiv.org/pdf/2601.08464)
- [More Images, More Problems? A Controlled Analysis of VLM Failure Modes](https://arxiv.org/pdf/2601.07812)
- [Smooth Operator: Smooth Verifiable Reward Activates Spatial Reasoning Ability of Vision-Language Model](https://arxiv.org/pdf/2601.07695)
- [Seeing Right but Saying Wrong: Inter- and Intra-Layer Refinement in MLLMs without Training](https://arxiv.org/pdf/2601.07359)
- [A Visual Semantic Adaptive Watermark grounded by Prefix-Tuning for Large Vision-Language Model](https://arxiv.org/pdf/2601.07291)
- [Mechanisms of Prompt-Induced Hallucination in Vision-Language Models](https://arxiv.org/pdf/2601.05201)
- [CoV: Chain-of-View Prompting for Spatial Reasoning](https://arxiv.org/pdf/2601.05172)
- [Vision-Language Introspection: Mitigating Overconfident Hallucinations in MLLMs via Interpretable Bi-Causal Steering](https://arxiv.org/pdf/2601.05159)
- [A Lightweight and Explainable Vision-Language Framework for Crop Disease Visual Question Answering](https://arxiv.org/pdf/2601.05143)
- [From Understanding to Engagement: Personalized pharmacy Video Clips via Vision Language Models (VLMs)](https://arxiv.org/pdf/2601.05059)
- [GeM-VG: Towards Generalized Multi-image Visual Grounding with Multimodal Large Language Models](https://arxiv.org/pdf/2601.04777)
- [Skeletonization-Based Adversarial Perturbations on Large Vision Language Model's Mathematical Text Recognition](https://arxiv.org/pdf/2601.04752)
- [GeoReason: Aligning Thinking And Answering In Remote Sensing Vision-Language Models Via Logical Consistency Reinforcement Learning](https://arxiv.org/pdf/2601.04118)
- [HemBLIP: A Vision-Language Model for Interpretable Leukemia Cell Morphology Analysis](https://arxiv.org/pdf/2601.03915)
- [Text-Guided Layer Fusion Mitigates Hallucination in Multimodal LLMs](https://arxiv.org/pdf/2601.03100)
- [ReCCur: A Recursive Corner-Case Curation Framework for Robust Vision-Language Understanding in Open and Edge Scenarios](https://arxiv.org/pdf/2601.03011)
- [Towards Faithful Reasoning in Comics for Small MLLMs](https://arxiv.org/pdf/2601.02991)
- [EarthVL: A Progressive Earth Vision-Language Understanding and Generation Framework](https://arxiv.org/pdf/2601.02783)
- [AbductiveMLLM: Boosting Visual Abductive Reasoning Within MLLMs](https://arxiv.org/pdf/2601.02771)
- [Unveiling and Bridging the Functional Perception Gap in MLLMs: Atomic Visual Alignment and Hierarchical Evaluation via PET-Bench](https://arxiv.org/pdf/2601.02737)
- [BiPrompt: Bilateral Prompt Optimization for Visual and Textual Debiasing in Vision-Language Models](https://arxiv.org/pdf/2601.02147)
- [Leveraging 2D-VLM for Label-Free 3D Segmentation in Large-Scale Outdoor Scene Understanding](https://arxiv.org/pdf/2601.02029)
- [Thinking with Blueprints: Assisting Vision-Language Models in Spatial Reasoning via Structured Object Representation](https://arxiv.org/pdf/2601.01984)
- [AFTER: Mitigating the Object Hallucination of LVLM via Adaptive Factual-Guided Activation Editing](https://arxiv.org/pdf/2601.01957)
- [Grading Handwritten Engineering Exams with Multimodal Large Language Models](https://arxiv.org/pdf/2601.00730)
- [Detecting Performance Degradation under Data Shift in Pathology Vision-Language Model](https://arxiv.org/pdf/2601.00716)
- [CRoPS: A Training-Free Hallucination Mitigation Framework for Vision-Language Models](https://arxiv.org/pdf/2601.00659)
- [CPPO: Contrastive Perception for Vision Language Policy Optimization](https://arxiv.org/pdf/2601.00501)
- [FaithSCAN: Model-Driven Single-Pass Hallucination Detection for Faithful Visual Question Answering](https://arxiv.org/pdf/2601.00269)
